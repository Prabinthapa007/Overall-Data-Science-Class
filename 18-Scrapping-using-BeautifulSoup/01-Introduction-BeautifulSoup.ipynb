{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2ffc18-361b-45e1-bc2f-a785ded4c63c",
   "metadata": {},
   "source": [
    "There are mainly two ways to extract data from a website:\n",
    "- Use of API of the website (if it exists). For example, Facebook has the Facebook Graph API which allows retrieval of data posted on Facebook.\n",
    "- Access the HTML of the webpage and extract useful information/data from it. This technique is called web scrapping or web harvesting or web data extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9895850-14ca-423d-b280-a5b53d82ecf5",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "`BeautifulSoup` is a Python library used for web scrapping purposes to pull the data out of HTML and XML files. It creates parse trees from page source codes that can be used to extract data easily. BeautifulSoup provides Pythonic idioms for iterating, searching, and modifying the parse tree, which makes it easier to work with web data.\n",
    "\n",
    "## Key Features:\n",
    "- **HTML Parsing**: Handles broken HTML and creates a parse tree.\n",
    "- **Navigation**: Navigate the parse tree using tag names, attributes, and text.\n",
    "- **Search**: Find specific elements using various search methods.\n",
    "- **Modification**: Modify the parse tree by adding, removing, or changing tags.\n",
    "\n",
    "\n",
    "## Steps involves on How to use BeautifulSoup to scrape a web page.\n",
    "1. Send an HTTP request to the URL of the webpage you want to access. The server responds to the request by returning the HTML content of the webpage. For this task, we will use a third-party HTTP library for python-requests.\n",
    "\n",
    "2. Once we have accessed the HTML content, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available but the most advanced one is html5lib.\n",
    "\n",
    "3. Now, all we need to do is navigating and searching the parse tree that we created, i.e. tree traversal. For this task, we will be using another third-party python library, Beautiful Soup. It is a Python library for pulling data out of HTML and XML files.\n",
    "\n",
    "\n",
    "**Step 1: Install BeautifulSoup and requests**\n",
    "```python\n",
    "pip install beautifulsoup4\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "**Step 2: Import Libraries**\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "**Step 3: Send a Request to a Web Page**\n",
    "Send a request to the web page you want to scrape:\n",
    "```python\n",
    "url = 'https://example.com'\n",
    "response = requests.get(url)\n",
    "```\n",
    "\n",
    "**Step 4: Parse the HTML Content**\n",
    "Parse the HTML content using BeautifulSoup:\n",
    "```python\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "\n",
    "**Step 5: Extract Data**\n",
    "Extract specific data from the web page. For example, let's extract all the titles from the page:\n",
    "```python\n",
    "titles = soup.find_all('h1') # Assuming titles are within <h1> tags\n",
    "for title in titles:\n",
    "    print(title.get_text())\n",
    "```\n",
    "\n",
    "## Explanation:\n",
    "- **requests.get(url)**: Sends a GET request to the specified URL and retrieves the content.\n",
    "- **BeautifulSoup(response.content, 'html.parser')**: Parses the HTML content using the 'html.parser' parser.\n",
    "- **soup,find_all('h1')**: Finds all the `<h1>` tags in the HTML and returns them as a list.\n",
    "- **title.get_text()**: Extracts the text content from each `<h1>` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc998d77-c825-4bc3-9eaf-e93bc9dc3101",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293b400-51f3-4958-be44-4c37039d6e63",
   "metadata": {},
   "source": [
    "# Responses \n",
    "## **1. <Response [200]>**:\n",
    "A `200 OK` response means that the request was successful and the sever has returned the requested resource. This is the standard response for sucessful HTTP requests. \n",
    "\n",
    "## **2. <Response [403]>**: \n",
    "A `403 Forbidden` response status code indicates that the server understood the request but refuses to authorize it. This can happen for several reasons, such as insufficient permissions, IP blocking, or restrictions on automated access (common with web scrapping).\n",
    "\n",
    "### Handling HTTP 403 Forbidden Error in Web Scraping with BeautifulSoup\n",
    "#### 1. **Check the URL**:\n",
    "Make sure the URL you are trying to access is correct and does not require additional parameters or headers.\n",
    "   \n",
    "#### 2. **User-Agent Header**:\n",
    "Some websites block requests that do not have a proper User-Agent Header. Adding a User-Agent header can help bypass this restrition:\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page we want to scrape\n",
    "url = 'https://example.com'\n",
    "\n",
    "# Set the headers to make the request look like it is coming from a browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Send a GET request to the URL with headers\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find all <h1> tags and extract their text\n",
    "    titles = soup.find_all('h1')\n",
    "    for title in titles:\n",
    "        print(title.get_text())\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "```\n",
    "\n",
    "#### 3. Verify Access Restrictions:\n",
    "Some websites restrict access based on geographic location or IP addresses. You might try using a proxy or VPN to see if it resolves the issue.\n",
    "\n",
    "\n",
    "#### 4. Check for Additional Headers or Cookies:\n",
    "Some websites require additional headers or cookies to be set. Inspect the network requests in your browser's developer tools to see if there are any such requirements.\n",
    "\n",
    "#### 5. Respect Robots.txt:\n",
    "Ensure that the website allows web scrapping by checking its `robots.txt` file. Accessing restricted parts of the website might cause a `403 Forbidden` response.\n",
    "\n",
    "#### 6. Use a Web Scraping API\n",
    "If manual scraping is difficult, you might consider using a web scrapping API like ScraperAPI, ScrapingBee, or others that handle these issues for you.\n",
    "\n",
    "#### Retry with Autnentication\n",
    "If ethe website requires login credentials, you may need to handle authentication:\n",
    "```python\n",
    "# Example for basic authentication\n",
    "response = requests.get(url, headers=headers, auth('username', 'password'))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7283615-5036-4a43-bbcd-9e3b6eba15cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
